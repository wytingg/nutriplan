# NutriPlan Multi-Task Training Configuration

# Model Configuration
model:
  name: "meta-llama/Llama-3.2-3B"  # Options: Llama-3, Qwen-2, Mistral, Gemma-2, Yi-1.5
  max_length: 2048
  fp16: true
  gradient_checkpointing: false

# Data Configuration
data:
  data_dir: "D:\\Downloads"
  num_workers: 4

# Task Ratios (must sum to 1.0)
tasks:
  task_a_ratio: 0.5  # Discriminative Ranking
  task_b_ratio: 0.3  # Constrained Generation
  task_c_ratio: 0.2  # Reflective Editing

# Training Configuration
training:
  num_epochs: 5
  batch_size: 8
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0

# Early Stopping
early_stopping:
  patience: 3
  metric: "val_loss"
  mode: "min"

# Checkpointing
checkpoint:
  output_dir: "checkpoints/nutriplan"
  save_strategy: "epoch"
  save_total_limit: 3
  load_best_model_at_end: true

# Logging
logging:
  use_wandb: true
  wandb_project: "nutriplan"
  run_name: "nutriplan_full"
  logging_steps: 10

# Hardware
hardware:
  multi_gpu: true
  num_gpus: 8

# Reproducibility
seed: 42

# Evaluation
evaluation:
  eval_strategy: "epoch"
  metrics:
    - "sncr"
    - "upm"
    - "k_faith"
    - "avc"
    - "dist_2"
    - "bleu"
    - "rouge_l"
    - "nutrition_accuracy"
