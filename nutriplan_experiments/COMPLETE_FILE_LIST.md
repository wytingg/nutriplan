# Complete File List - NutriPlan Experiments (Stages I-III)

This document lists all files created for the first three experimental stages.

## Summary

**Total NEW files created for Stages I.5, II, III:** 9 files
**Documentation files:** 3 files
**Total:** 12 new files

---

## NEW Files for Stages I.5, II, III

### 1. Stage I.5: Hyperparameter Search

**File:** `scripts/hyperparameter_search.py` (240 lines)
- **Purpose:** Grid or random search over hyperparameter space
- **Key Features:**
  - Supports grid search and random search
  - Validates on validation set
  - Saves best configuration to `best_config.json`
- **Usage:**
```bash
python scripts/hyperparameter_search.py \
    --model_name meta-llama/Llama-3.2-3B \
    --search_space configs/search_space.yaml \
    --data_dir D:/Downloads \
    --output_dir experiments/hyperparam_search \
    --search_type random
```

**File:** `configs/search_space.yaml` (59 lines)
- **Purpose:** Defines hyperparameter search space
- **Parameters:** learning_rate, batch_size, num_epochs, warmup_ratio, weight_decay, max_grad_norm, task ratios

---

### 2. Stage II (RQ1): Base LLM Selection

**File:** `scripts/train_all_llms.sh` (110 lines)
- **Purpose:** Batch training script for 5 base LLMs × 3 seeds = 15 experiments
- **Models:** Llama-3.2-3B, Llama-3-8B, Qwen2-7B, Mistral-7B-v0.3, Gemma-2-9b
- **Seeds:** 42, 123, 2024
- **Usage:**
```bash
chmod +x scripts/train_all_llms.sh
bash scripts/train_all_llms.sh
```

**File:** `scripts/aggregate_rq1_results.py` (233 lines)
- **Purpose:** Aggregate multi-seed results and generate Table X
- **Output:** `results/table_x.txt`, `.csv`, `.tex`
- **Usage:**
```bash
python scripts/aggregate_rq1_results.py \
    --experiments_dir experiments \
    --models meta-llama/Llama-3.2-3B meta-llama/Llama-3-8B Qwen/Qwen2-7B mistralai/Mistral-7B-v0.3 google/gemma-2-9b \
    --seeds 42 123 2024 \
    --output_file results/table_x.txt
```

---

### 3. Stage III (RQ2): Overall Performance Comparison

**File:** `scripts/run_rq2_experiments.sh` (160 lines)
- **Purpose:** Run all baseline comparisons (Retrieval, RAG, SFT, Zero-shot)
- **Key Feature:** Auto-generates baseline config and Table Y
- **Usage:**
```bash
# First update BEST_BASE_LLM based on Table X results
chmod +x scripts/run_rq2_experiments.sh
bash scripts/run_rq2_experiments.sh
```

**File:** `scripts/generate_table_y.py` (242 lines)
- **Purpose:** Aggregate RQ2 results and generate comparison table
- **Output:** `results/table_y.txt`, `.csv`, `.tex`
- **Features:** Statistical significance testing, effect sizes
- **Usage:**
```bash
python scripts/generate_table_y.py \
    --experiments_dir experiments \
    --baseline_config configs/rq2_baseline_config.json \
    --output_file results/table_y.txt \
    --nutriplan_name NutriPlan
```

**File:** `configs/rq2_baseline_config_template.json` (20 lines)
- **Purpose:** Configuration mapping baselines to their evaluation directories
- **Note:** Auto-generated by `run_rq2_experiments.sh`

---

### 4. Supporting Utilities

**File:** `utils/kg_utils.py` (328 lines)
- **Purpose:** Knowledge Graph accessor using graph-tool
- **Key Classes:**
  - `NutriPlanKG`: Main KG interface
- **Key Methods:**
  - `get_recipe_by_id()`: Retrieve recipe node
  - `get_user_rni_targets()`: Get user nutrition targets
  - `get_ingredient_cooccurrence()`: Ingredient co-occurrence patterns
  - `get_recommended_ingredients_for_user()`: KG-based recommendations
- **Usage:**
```python
from utils.kg_utils import NutriPlanKG
kg = NutriPlanKG(kg_path="work/recipebench/kg/nutriplan_kg4.graphml")
recipe = kg.get_recipe_by_id("recipe_123")
```

**File:** `utils/statistical_tests.py` (290 lines)
- **Purpose:** Statistical significance testing utilities
- **Key Classes:**
  - `SignificanceTests`: Statistical test suite
- **Key Methods:**
  - `paired_t_test()`: Paired t-test
  - `wilcoxon_test()`: Non-parametric alternative
  - `effect_size_cohens_d()`: Effect size calculation
  - `bootstrap_confidence_interval()`: Bootstrap CI
  - `format_p_value()`: Add significance stars (*, **, ***)
- **Usage:**
```python
from utils.statistical_tests import SignificanceTests
tester = SignificanceTests()
t_stat, p_value = tester.paired_t_test(values_a, values_b)
```

**File:** `baselines/zero_shot.py` (345 lines)
- **Purpose:** Zero-shot LLM baseline using prompt engineering
- **Key Features:**
  - Task B: Constrained recipe generation
  - Task C: Recipe editing
  - JSON output parsing
- **Usage:**
```bash
# Task B
python baselines/zero_shot.py \
    --test_file D:/Downloads/task_b_test_from_kg.jsonl \
    --model_name meta-llama/Llama-3-8B \
    --output_file experiments/rq2_zeroshot/task_b_predictions.jsonl \
    --task b

# Task C
python baselines/zero_shot.py \
    --test_file D:/Downloads/task_c_test_from_kg.jsonl \
    --model_name meta-llama/Llama-3-8B \
    --output_file experiments/rq2_zeroshot/task_c_predictions.jsonl \
    --task c
```

---

### 5. Documentation

**File:** `EXECUTION_GUIDE.md` (550+ lines)
- **Purpose:** Complete step-by-step execution guide
- **Contents:**
  - Prerequisites and environment setup
  - Stage-by-stage instructions with commands
  - Expected outputs and time estimates
  - Troubleshooting section
  - Performance benchmarks

**File:** `README.md` (Updated)
- **Purpose:** Project overview and quick start
- **Contents:**
  - Project structure
  - Quick start commands
  - Evaluation metrics
  - Hardware requirements

**File:** `COMPLETE_FILE_LIST.md` (This file)
- **Purpose:** Complete list of all created files
- **Contents:**
  - File-by-file description
  - Usage examples
  - Complete execution sequence

---

## Previously Existing Files (From Stage I)

These files were created in earlier stages and are required:

### Data Processing
- `data/dataset.py` - Multi-task dataset loader
- `data/data_statistics.py` - Original data statistics (has format issues)
- `data/data_statistics_version1.py` - Fixed data statistics

### Training
- `training/run_nutriplan.py` - Main NutriPlan training script
- `training/train_sft.py` - SFT baseline training

### Evaluation
- `evaluation/metrics.py` - 8 evaluation metrics implementation
- `evaluation/evaluation.py` - Main evaluator class
- `evaluation/run_evaluation.py` - Evaluation runner script

### Baselines
- `baselines/retrieval.py` - BM25 + User Profile Similarity
- `baselines/rag.py` - Retrieval-Augmented Generation

### Configurations
- `configs/nutriplan_config.yaml` - NutriPlan configuration
- `configs/baseline_configs.yaml` - Baseline configurations

### Utilities
- `utils/logger.py` - Logging utilities
- `utils/nutrition_calculator.py` - Nutrition calculation

---

## Complete Execution Sequence

### Step 1: Environment Setup (One-time)

```bash
# On your server
conda create -n nutriplan python=3.10
conda activate nutriplan

# Install dependencies
pip install torch transformers datasets
pip install pandas numpy scipy scikit-learn
pip install nltk rouge-score tqdm pyyaml

# Install graph-tool (conda recommended)
conda install -c conda-forge graph-tool

# Download NLTK data
python -c "import nltk; nltk.download('punkt')"
```

### Step 2: Verify Data and KG

```bash
# Check data files exist
ls D:/Downloads/task_a_train_discriminative.jsonl
ls D:/Downloads/task_b_train_from_kg.jsonl
ls D:/Downloads/task_c_train_from_kg.jsonl

# Check KG file exists
ls work/recipebench/kg/nutriplan_kg4.graphml

# Test KG loading
python -c "from utils.kg_utils import NutriPlanKG; kg = NutriPlanKG(); print('KG loaded successfully')"
```

### Step 3: Run Data Statistics (Optional - for verification)

```bash
python data/data_statistics_version1.py \
    --data_dir D:/Downloads \
    --output_dir results/data_stats
```

**Expected Output:**
- Total: 38,160 samples
- Task A: 14,000, Task B: 14,000, Task C: 11,160

### Step 4: Stage I.5 - Hyperparameter Search (~4-8 hours)

```bash
python scripts/hyperparameter_search.py \
    --model_name meta-llama/Llama-3.2-3B \
    --search_space configs/search_space.yaml \
    --data_dir D:/Downloads \
    --output_dir experiments/hyperparam_search \
    --search_type random
```

**Output:** `experiments/hyperparam_search/best_config.json`

**Action:** Review best config and update hyperparameters in `train_all_llms.sh` if needed (lines 16-21)

### Step 5: Stage II - Train All Base LLMs (~3-7 days)

```bash
# Make executable
chmod +x scripts/train_all_llms.sh

# Run batch training (5 models × 3 seeds = 15 experiments)
bash scripts/train_all_llms.sh
```

**Output:** 15 trained models in `experiments/rq1_{model}_seed_{seed}/`

**Monitor Progress:**
```bash
# Check completed experiments
find experiments -name "training_complete.txt"

# Check training logs
tail -f experiments/rq1_meta-llama_Llama-3-8B_seed_42/logs/train.log
```

### Step 6: Aggregate RQ1 Results - Generate Table X (~1-2 minutes)

```bash
python scripts/aggregate_rq1_results.py \
    --experiments_dir experiments \
    --models meta-llama/Llama-3.2-3B meta-llama/Llama-3-8B Qwen/Qwen2-7B mistralai/Mistral-7B-v0.3 google/gemma-2-9b \
    --seeds 42 123 2024 \
    --output_file results/table_x.txt
```

**Output:**
- `results/table_x.txt` (formatted table)
- `results/table_x.csv` (CSV)
- `results/table_x.tex` (LaTeX)

**Action:** Review Table X and identify best base LLM by SNCR metric

### Step 7: Update Best Base LLM for RQ2

```bash
# Edit run_rq2_experiments.sh line 11
# Change BEST_BASE_LLM to your best model from Table X
# For example:
# BEST_BASE_LLM="meta-llama/Llama-3-8B"

nano scripts/run_rq2_experiments.sh
# or
vim scripts/run_rq2_experiments.sh
```

### Step 8: Stage III - Run All Baseline Comparisons (~1-2 days)

```bash
# Make executable
chmod +x scripts/run_rq2_experiments.sh

# Run all RQ2 experiments
bash scripts/run_rq2_experiments.sh
```

**This script will:**
1. Run Retrieval baseline (~30-60 min)
2. Run RAG baseline (~2-4 hours)
3. Train and evaluate SFT baseline (~4-8 hours)
4. Run Zero-shot baseline (~3-6 hours)
5. Evaluate NutriPlan (if not done)
6. Generate baseline config
7. Generate Table Y

**Output:**
- `experiments/rq2_retrieval/eval/`
- `experiments/rq2_rag/eval/`
- `experiments/rq2_sft/eval/`
- `experiments/rq2_zeroshot/eval/`
- `configs/rq2_baseline_config.json` (auto-generated)
- `results/table_y.txt`, `.csv`, `.tex`

### Step 9: Review Results

```bash
# View Table X (RQ1: Base LLM Selection)
cat results/table_x.txt

# View Table Y (RQ2: Overall Comparison)
cat results/table_y.txt

# View detailed metrics
cat experiments/rq1_meta-llama_Llama-3-8B_seed_42/eval/aggregate_metrics.json
```

---

## File Size Summary

| Category | Files | Total Lines | Purpose |
|----------|-------|-------------|---------|
| Scripts | 5 | ~990 | Automation and aggregation |
| Utilities | 3 | ~963 | KG access, statistics, zero-shot |
| Configs | 2 | ~79 | Search space, baseline config |
| Documentation | 3 | ~1500 | Guides and references |
| **Total** | **13** | **~3532** | **Stages I.5, II, III** |

---

## Expected Output Files After Completion

```
experiments/
├── hyperparam_search/
│   ├── best_config.json              # Best hyperparameters
│   └── search_results.json           # All trial results
│
├── rq1_meta-llama_Llama-3.2-3B_seed_42/
│   ├── best_model/                   # Trained model
│   ├── eval/
│   │   └── aggregate_metrics.json    # Evaluation results
│   └── training_complete.txt
│
├── rq1_meta-llama_Llama-3-8B_seed_42/
│   └── ... (same structure)
│
├── ... (13 more rq1 experiments)
│
├── rq2_retrieval/eval/
├── rq2_rag/eval/
├── rq2_sft/eval/
└── rq2_zeroshot/eval/

results/
├── table_x.txt                       # RQ1 comparison (text)
├── table_x.csv                       # RQ1 comparison (CSV)
├── table_x.tex                       # RQ1 comparison (LaTeX)
├── table_y.txt                       # RQ2 comparison (text)
├── table_y.csv                       # RQ2 comparison (CSV)
└── table_y.tex                       # RQ2 comparison (LaTeX)

configs/
└── rq2_baseline_config.json          # Auto-generated baseline config
```

---

## Time Estimates

| Stage | Task | Time | GPU Required |
|-------|------|------|--------------|
| I.5 | Hyperparameter Search (random, 20 trials) | 4-8 hours | Yes (16GB+) |
| II | Train 15 models (5 models × 3 seeds) | 3-7 days | Yes (24GB+ recommended) |
| II | Aggregate RQ1 results | 1-2 minutes | No |
| III | Retrieval baseline | 30-60 minutes | No |
| III | RAG baseline | 2-4 hours | Yes |
| III | SFT baseline | 4-8 hours | Yes |
| III | Zero-shot baseline | 3-6 hours | Yes |
| III | Generate Table Y | 1-2 minutes | No |
| **Total** | **Full Pipeline** | **~1-2 weeks** | **Yes** |

---

## Troubleshooting Quick Reference

### GPU Out of Memory
```bash
# Reduce batch size in train_all_llms.sh (line 17)
BATCH_SIZE=4  # Instead of 8

# Or use smaller model first
meta-llama/Llama-3.2-3B  # 16GB VRAM
```

### KG Loading Issues
```bash
# Install graph-tool via conda
conda install -c conda-forge graph-tool

# Verify KG file
python -c "from utils.kg_utils import NutriPlanKG; kg = NutriPlanKG()"
```

### Script Permission Denied
```bash
chmod +x scripts/*.sh
```

### WandB Issues
```bash
# Login (if using --use_wandb)
wandb login

# Or remove --use_wandb from train_all_llms.sh (line 48)
```

---

## Next Steps After Stage III

1. **Analyze Tables X and Y** for paper results
2. **Stage IV: Ablation Studies** (component analysis)
3. **Stage V: Generalization Testing** (cross-domain)
4. **Stage VI: Human Evaluation** (recipe quality)

---

## Contact & Support

For detailed instructions, see:
- [`EXECUTION_GUIDE.md`](EXECUTION_GUIDE.md) - Step-by-step guide
- [`README.md`](README.md) - Project overview
- Code comments in source files

**All files are ready to transfer to your server and run!**

---

**Last Updated:** 2025-01-XX
**Stage Coverage:** I.5, II (RQ1), III (RQ2)
**Status:** Complete and ready for server deployment
