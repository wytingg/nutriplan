#!/usr/bin/env python3
"""
è¶…çº§è¯¦ç»†çš„è®­ç»ƒæ•°æ®è¯Šæ–­è„šæœ¬
æ£€æŸ¥æ‰€æœ‰å¯èƒ½å½±å“ LLM è®­ç»ƒè´¨é‡çš„é—®é¢˜
"""

import json
import sys
import re
from pathlib import Path
from collections import Counter, defaultdict
import unicodedata

class DataDiagnostics:
    def __init__(self, filepath):
        self.filepath = filepath
        self.data = []
        self.issues = []
        self.warnings = []
        self.stats = defaultdict(list)

    def load_data(self):
        """åŠ è½½ JSONL æ•°æ®"""
        print(f"\n{'='*80}")
        print(f"æ£€æŸ¥æ–‡ä»¶: {self.filepath}")
        print(f"{'='*80}\n")

        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if line.strip():
                        try:
                            self.data.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            self.issues.append(f"ç¬¬ {i+1} è¡Œ JSON è§£æé”™è¯¯: {e}")
        except Exception as e:
            self.issues.append(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
            return False

        print(f"âœ“ åŠ è½½äº† {len(self.data)} æ¡æ•°æ®")
        return True

    def check_basic_format(self):
        """æ£€æŸ¥åŸºæœ¬æ ¼å¼"""
        print("\n[1/12] æ£€æŸ¥åŸºæœ¬å­—æ®µ...")

        required_fields = ['instruction', 'output']

        for i, item in enumerate(self.data):
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            missing = [f for f in required_fields if f not in item]
            if missing:
                self.issues.append(f"æ ·æœ¬ {i}: ç¼ºå°‘å­—æ®µ {missing}")

            # æ£€æŸ¥å­—æ®µæ˜¯å¦ä¸ºç©º
            for field in required_fields:
                if field in item and not item[field]:
                    self.issues.append(f"æ ·æœ¬ {i}: å­—æ®µ '{field}' ä¸ºç©º")

        if not self.issues:
            print("  âœ“ æ‰€æœ‰æ ·æœ¬éƒ½åŒ…å«å¿…éœ€å­—æ®µ")

    def check_encoding(self):
        """æ£€æŸ¥ç¼–ç é—®é¢˜"""
        print("\n[2/12] æ£€æŸ¥å­—ç¬¦ç¼–ç ...")

        non_ascii_samples = []
        control_char_samples = []

        for i, item in enumerate(self.data):
            text = json.dumps(item, ensure_ascii=False)

            # æ£€æŸ¥é ASCII å­—ç¬¦ï¼ˆé™¤äº†æ­£å¸¸çš„æ ‡ç‚¹ï¼‰
            non_ascii = [c for c in text if ord(c) > 127]
            if non_ascii:
                # ç»Ÿè®¡å­—ç¬¦ç±»å‹
                char_types = Counter([unicodedata.category(c) for c in non_ascii])
                # å¦‚æœåŒ…å«è¥¿é‡Œå°”æ–‡ã€ä¸­æ–‡ç­‰
                if any(ord(c) > 0x0400 for c in non_ascii):
                    non_ascii_samples.append({
                        'index': i,
                        'chars': ''.join(set(non_ascii))[:50],
                        'categories': dict(char_types)
                    })

            # æ£€æŸ¥æ§åˆ¶å­—ç¬¦
            control_chars = [c for c in text if unicodedata.category(c) == 'Cc' and c not in '\n\t']
            if control_chars:
                control_char_samples.append(i)

        if non_ascii_samples:
            self.warnings.append(f"å‘ç° {len(non_ascii_samples)} ä¸ªæ ·æœ¬åŒ…å«é ASCII å­—ç¬¦")
            for sample in non_ascii_samples[:5]:
                print(f"  âš ï¸  æ ·æœ¬ {sample['index']}: åŒ…å«å­—ç¬¦ '{sample['chars']}'")
        else:
            print("  âœ“ æ‰€æœ‰æ ·æœ¬éƒ½æ˜¯çº¯ ASCIIï¼ˆè‹±æ–‡ï¼‰")

        if control_char_samples:
            self.issues.append(f"{len(control_char_samples)} ä¸ªæ ·æœ¬åŒ…å«æ§åˆ¶å­—ç¬¦")

    def check_length_distribution(self):
        """æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ"""
        print("\n[3/12] æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ...")

        inst_lengths = []
        out_lengths = []

        for item in self.data:
            inst_lengths.append(len(item.get('instruction', '')))
            out_lengths.append(len(item.get('output', '')))

        self.stats['inst_length'] = inst_lengths
        self.stats['out_length'] = out_lengths

        print(f"  Instruction é•¿åº¦: min={min(inst_lengths)}, max={max(inst_lengths)}, avg={sum(inst_lengths)/len(inst_lengths):.1f}")
        print(f"  Output é•¿åº¦:      min={min(out_lengths)}, max={max(out_lengths)}, avg={sum(out_lengths)/len(out_lengths):.1f}")

        # æ£€æŸ¥å¼‚å¸¸çŸ­çš„æ ·æœ¬
        short_inst = [i for i, l in enumerate(inst_lengths) if l < 50]
        short_out = [i for i, l in enumerate(out_lengths) if l < 20]

        if short_inst:
            self.warnings.append(f"{len(short_inst)} ä¸ªæ ·æœ¬çš„ instruction è¿‡çŸ­ (<50 å­—ç¬¦)")
        if short_out:
            self.warnings.append(f"{len(short_out)} ä¸ªæ ·æœ¬çš„ output è¿‡çŸ­ (<20 å­—ç¬¦)")

        # æ£€æŸ¥å¼‚å¸¸é•¿çš„æ ·æœ¬
        long_inst = [i for i, l in enumerate(inst_lengths) if l > 2000]
        long_out = [i for i, l in enumerate(out_lengths) if l > 4000]

        if long_inst:
            self.warnings.append(f"{len(long_inst)} ä¸ªæ ·æœ¬çš„ instruction è¿‡é•¿ (>2000 å­—ç¬¦)")
        if long_out:
            self.warnings.append(f"{len(long_out)} ä¸ªæ ·æœ¬çš„ output è¿‡é•¿ (>4000 å­—ç¬¦)")

    def check_duplicates(self):
        """æ£€æŸ¥é‡å¤æ ·æœ¬"""
        print("\n[4/12] æ£€æŸ¥é‡å¤æ ·æœ¬...")

        inst_hashes = {}
        duplicates = []

        for i, item in enumerate(self.data):
            inst = item.get('instruction', '')
            if inst in inst_hashes:
                duplicates.append((i, inst_hashes[inst]))
            else:
                inst_hashes[inst] = i

        if duplicates:
            self.warnings.append(f"å‘ç° {len(duplicates)} å¯¹é‡å¤çš„ instruction")
            for dup in duplicates[:3]:
                print(f"  âš ï¸  æ ·æœ¬ {dup[0]} å’Œ {dup[1]} çš„ instruction ç›¸åŒ")
        else:
            print("  âœ“ æ²¡æœ‰é‡å¤çš„ instruction")

    def check_format_consistency(self):
        """æ£€æŸ¥æ ¼å¼ä¸€è‡´æ€§"""
        print("\n[5/12] æ£€æŸ¥ output æ ¼å¼ä¸€è‡´æ€§...")

        # æ£€æŸ¥ output æ ¼å¼æ¨¡å¼
        patterns = {
            'numbered_list': r'^\d+\.\s',  # 1. 2. 3.
            'markdown_bold': r'\*\*.*?\*\*',  # **text**
            'recipe_format': r'Ingredients:|Instructions:|Nutrition:',
            'diagnosis_format': r'Diagnosis:|Corrections:',
            'json_format': r'^\s*\{',
        }

        format_counts = Counter()

        for item in self.data:
            output = item.get('output', '')
            for fmt_name, pattern in patterns.items():
                if re.search(pattern, output, re.MULTILINE):
                    format_counts[fmt_name] += 1

        print("  Output æ ¼å¼åˆ†å¸ƒ:")
        total = len(self.data)
        for fmt, count in format_counts.most_common():
            print(f"    - {fmt}: {count} ({count/total*100:.1f}%)")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆæ ¼å¼
        if len(format_counts) > 3:
            self.warnings.append(f"æ£€æµ‹åˆ° {len(format_counts)} ç§ä¸åŒçš„ output æ ¼å¼ï¼Œå¯èƒ½å½±å“è®­ç»ƒä¸€è‡´æ€§")

    def check_special_tokens(self):
        """æ£€æŸ¥ç‰¹æ®Š token å’Œæ ‡è®°"""
        print("\n[6/12] æ£€æŸ¥ç‰¹æ®Š token...")

        special_patterns = [
            (r'<[^>]+>', 'HTML/XML æ ‡ç­¾'),
            (r'\[.*?\]', 'æ–¹æ‹¬å·æ ‡è®°'),
            (r'\{.*?\}', 'èŠ±æ‹¬å·ï¼ˆå¯èƒ½æ˜¯ JSONï¼‰'),
            (r'@\w+', '@mention'),
            (r'#\w+', 'hashtag'),
            (r'http[s]?://\S+', 'URL'),
        ]

        for pattern, name in special_patterns:
            count = 0
            for item in self.data:
                text = json.dumps(item, ensure_ascii=False)
                if re.search(pattern, text):
                    count += 1
            if count > 0:
                print(f"    {name}: {count} ä¸ªæ ·æœ¬ ({count/len(self.data)*100:.1f}%)")

    def check_numeric_patterns(self):
        """æ£€æŸ¥æ•°å€¼æ¨¡å¼"""
        print("\n[7/12] æ£€æŸ¥è¥å…»æ•°å€¼...")

        # æå–è¥å…»æ•°å€¼
        nutrition_values = {
            'calories': [],
            'protein': [],
            'fiber': [],
            'sodium': []
        }

        for item in self.data:
            text = item.get('output', '') + item.get('instruction', '')

            # kcal
            cals = re.findall(r'(\d+)\s*kcal', text)
            if cals:
                nutrition_values['calories'].extend([int(c) for c in cals])

            # protein
            proteins = re.findall(r'(\d+)g?\s*protein', text)
            if proteins:
                nutrition_values['protein'].extend([int(p) for p in proteins])

            # fiber
            fibers = re.findall(r'(\d+)g?\s*fiber', text)
            if fibers:
                nutrition_values['fiber'].extend([int(f) for f in fibers])

        for nutrient, values in nutrition_values.items():
            if values:
                print(f"  {nutrient}: èŒƒå›´ {min(values)}-{max(values)}, å¹³å‡ {sum(values)/len(values):.1f}")

    def check_vocabulary(self):
        """æ£€æŸ¥è¯æ±‡è¡¨"""
        print("\n[8/12] æ£€æŸ¥è¯æ±‡è¡¨...")

        all_words = []
        for item in self.data:
            text = item.get('instruction', '') + ' ' + item.get('output', '')
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            all_words.extend(words)

        vocab = Counter(all_words)
        print(f"  æ€»è¯æ±‡é‡: {len(vocab)} ä¸ªå”¯ä¸€å•è¯")
        print(f"  æ€» token æ•°: {len(all_words)}")
        print(f"  æœ€å¸¸è§å•è¯: {vocab.most_common(10)}")

    def check_label_imbalance(self):
        """æ£€æŸ¥æ ‡ç­¾/ä»»åŠ¡ä¸å¹³è¡¡"""
        print("\n[9/12] æ£€æŸ¥ä»»åŠ¡ç±»å‹åˆ†å¸ƒ...")

        # å°è¯•è¯†åˆ«ä»»åŠ¡ç±»å‹
        task_types = Counter()

        for item in self.data:
            inst = item.get('instruction', '').lower()

            if 'rank' in inst or 'score' in inst:
                task_types['ranking'] += 1
            elif 'recipe' in inst and 'generate' in inst:
                task_types['generation'] += 1
            elif 'diagnose' in inst or 'fix' in inst or 'correct' in inst:
                task_types['correction'] += 1
            else:
                task_types['other'] += 1

        print("  ä»»åŠ¡ç±»å‹åˆ†å¸ƒ:")
        for task, count in task_types.most_common():
            print(f"    - {task}: {count} ({count/len(self.data)*100:.1f}%)")

        # æ£€æŸ¥ä¸å¹³è¡¡
        if task_types:
            max_count = max(task_types.values())
            min_count = min(task_types.values())
            ratio = max_count / min_count if min_count > 0 else float('inf')

            if ratio > 5:
                self.warnings.append(f"ä»»åŠ¡ç±»å‹ä¸¥é‡ä¸å¹³è¡¡ï¼ˆæœ€å¤§/æœ€å°æ¯”ä¾‹: {ratio:.1f}ï¼‰")

    def check_whitespace_issues(self):
        """æ£€æŸ¥ç©ºç™½å­—ç¬¦é—®é¢˜"""
        print("\n[10/12] æ£€æŸ¥ç©ºç™½å­—ç¬¦...")

        issues_found = []

        for i, item in enumerate(self.data):
            for field in ['instruction', 'output']:
                text = item.get(field, '')

                # å¤šä½™çš„ç©ºç™½
                if '  ' in text:  # åŒç©ºæ ¼
                    issues_found.append(f"æ ·æœ¬ {i} {field}: åŒ…å«å¤šä½™ç©ºæ ¼")
                    break

                # å‰åç©ºç™½
                if text != text.strip():
                    issues_found.append(f"æ ·æœ¬ {i} {field}: å‰åæœ‰å¤šä½™ç©ºç™½")
                    break

                # Tab å­—ç¬¦
                if '\t' in text:
                    issues_found.append(f"æ ·æœ¬ {i} {field}: åŒ…å« Tab å­—ç¬¦")
                    break

        if issues_found:
            self.warnings.append(f"{len(issues_found)} ä¸ªæ ·æœ¬å­˜åœ¨ç©ºç™½å­—ç¬¦é—®é¢˜")
            for issue in issues_found[:5]:
                print(f"  âš ï¸  {issue}")
        else:
            print("  âœ“ ç©ºç™½å­—ç¬¦æ­£å¸¸")

    def check_instruction_output_mismatch(self):
        """æ£€æŸ¥ instruction å’Œ output çš„åŒ¹é…åº¦"""
        print("\n[11/12] æ£€æŸ¥ instruction-output åŒ¹é…...")

        mismatches = []

        for i, item in enumerate(self.data):
            inst = item.get('instruction', '').lower()
            out = item.get('output', '').lower()

            # æ£€æŸ¥ instruction è¦æ±‚æ’åºï¼Œä½† output æ²¡æœ‰æ•°å­—åˆ—è¡¨
            if ('rank' in inst or 'sort' in inst) and not re.search(r'^\d+\.', out, re.MULTILINE):
                mismatches.append(f"æ ·æœ¬ {i}: instruction è¦æ±‚æ’åºï¼Œä½† output æ— ç¼–å·åˆ—è¡¨")

            # æ£€æŸ¥ instruction è¦æ±‚é£Ÿè°±ï¼Œä½† output æ²¡æœ‰é£Ÿæ/æ­¥éª¤
            if 'recipe' in inst:
                if 'ingredient' not in out and 'instruction' not in out:
                    mismatches.append(f"æ ·æœ¬ {i}: instruction è¦æ±‚é£Ÿè°±ï¼Œä½† output ç¼ºå°‘ç»“æ„")

        if mismatches:
            self.warnings.append(f"{len(mismatches)} ä¸ªæ ·æœ¬çš„ instruction-output ä¸åŒ¹é…")
            for m in mismatches[:5]:
                print(f"  âš ï¸  {m}")
        else:
            print("  âœ“ instruction-output åŒ¹é…æ­£å¸¸")

    def check_potential_data_leakage(self):
        """æ£€æŸ¥æ½œåœ¨çš„æ•°æ®æ³„æ¼"""
        print("\n[12/12] æ£€æŸ¥æ½œåœ¨æ•°æ®æ³„æ¼...")

        leakage = []

        for i, item in enumerate(self.data):
            inst = item.get('instruction', '')
            out = item.get('output', '')

            # æ£€æŸ¥ output æ˜¯å¦åŒ…å«åœ¨ instruction ä¸­ï¼ˆé™¤äº†çŸ­è¯­ï¼‰
            if len(out) > 50 and out[:50] in inst:
                leakage.append(f"æ ·æœ¬ {i}: output çš„å¼€å¤´å‡ºç°åœ¨ instruction ä¸­")

        if leakage:
            self.issues.append(f"{len(leakage)} ä¸ªæ ·æœ¬å¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼")
            for l in leakage[:5]:
                print(f"  âŒ {l}")
        else:
            print("  âœ“ æœªæ£€æµ‹åˆ°æ•°æ®æ³„æ¼")

    def generate_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("è¯Šæ–­æŠ¥å‘Š")
        print(f"{'='*80}\n")

        if not self.issues and not self.warnings:
            print("ğŸ‰ æ­å–œï¼æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œæœªå‘ç°ä»»ä½•é—®é¢˜ï¼")
            return True

        if self.issues:
            print(f"âŒ å‘ç° {len(self.issues)} ä¸ªä¸¥é‡é—®é¢˜:")
            for issue in self.issues:
                print(f"  - {issue}")
            print()

        if self.warnings:
            print(f"âš ï¸  å‘ç° {len(self.warnings)} ä¸ªè­¦å‘Š:")
            for warning in self.warnings:
                print(f"  - {warning}")
            print()

        if self.issues:
            print("å»ºè®®: ä¿®å¤ä¸¥é‡é—®é¢˜åå†è®­ç»ƒ")
            return False
        else:
            print("å»ºè®®: è­¦å‘Šä¸å½±å“è®­ç»ƒï¼Œå¯ä»¥ç»§ç»­")
            return True

    def run_full_diagnostics(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        if not self.load_data():
            return False

        self.check_basic_format()
        self.check_encoding()
        self.check_length_distribution()
        self.check_duplicates()
        self.check_format_consistency()
        self.check_special_tokens()
        self.check_numeric_patterns()
        self.check_vocabulary()
        self.check_label_imbalance()
        self.check_whitespace_issues()
        self.check_instruction_output_mismatch()
        self.check_potential_data_leakage()

        return self.generate_report()


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python diagnose_data.py <file.jsonl>")
        sys.exit(1)

    filepath = sys.argv[1]

    if not Path(filepath).exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {filepath}")
        sys.exit(1)

    diagnostics = DataDiagnostics(filepath)
    success = diagnostics.run_full_diagnostics()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
