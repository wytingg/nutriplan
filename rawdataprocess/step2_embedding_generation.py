#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
step2_usda_align_plus.py â€” é£Ÿæâ†’USDA/FNDDS å¯¹é½ï¼ˆå¢å¼ºç‰ˆï¼‰

æ ¸å¿ƒæ”¹è¿›ï¼ˆä¸æ­¤å‰è®¨è®ºçš„â€œä¸ƒç‚¹æ”¹è¿› + åŒ¹é…ç‡æå‡ç­–ç•¥â€ä¸€è‡´ï¼‰ï¼š
1) è§„èŒƒåŒ–ä¸åŒä¹‰è¯è¡¨ï¼ˆB è¡¨ï¼‰ç»Ÿä¸€ï¼šå¤æ•°è¿˜åŸã€ä¿®é¥°è¯å‰¥ç¦»ã€åˆ«åæ˜ å°„ï¼ˆFoodOn/USDA åŒä¹‰è¯å¯å¹¶å…¥ï¼‰ã€‚
2) å¤šè·¯å¬å›ï¼šå€’æ’è¯å…¸ + RapidFuzz + ï¼ˆå¯é€‰ï¼‰Sentence-Transformers å‘é‡å¬å›ï¼ˆFAISS å¯é€‰ï¼‰ã€‚
3) å€™é€‰æ‰©å®¹ä¸é˜ˆå€¼è‡ªåŠ¨è°ƒåº¦ï¼štopKã€min_fuzz è‡ªé€‚åº”ï¼Œå¹¶ä¸ºé«˜é¢‘é£Ÿæé¢å¤–æ‰©å®¹ã€‚
4) é‡æ’åºæ‰“åˆ†å™¨ï¼šfuzz + embed_sim + dtype_prior + è§„åˆ™ä¸€è‡´æ€§ï¼ˆå•ä½/å½¢æ€ï¼‰ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–ä¸åŠ æƒã€‚
5) A è¡¨ï¼ˆfdc_id Ã— unit â†’ gramsï¼‰+ FNDDS household weights èåˆï¼šç”¨äºå…‹é‡å›å¡«ä¸ä¸€è‡´æ€§æ ¡éªŒã€‚
6) åŠ¨æ€é»˜è®¤å…‹é‡ï¼šæŒ‰å“ç±»ï¼ˆè”¬èœ/è°ƒå‘³/æ¶²ä½“/è›‹ç™½ç­‰ï¼‰ç»™å‡ºæ›´åˆç†çš„ default_grams è¦†ç›–ã€‚
7) é”™è¯¯æ„ŸçŸ¥è¿­ä»£ï¼šè¾“å‡º unmatched_topfreq.csv ä¸å¯¹é½å®¡è®¡ quick-reportï¼Œæ”¯æŒä¸‹ä¸€è½® B è¡¨å¢é‡å­¦ä¹ ã€‚

è¾“å…¥ï¼š
  --ingredients_csv       è§£æåçš„é…æ–™è¡¨ï¼ˆé•¿è¡¨ï¼Œæ”¯æŒCSVæˆ–Parquetæ ¼å¼ï¼Œè‡³å°‘åŒ…å« ingredient_norm / ingredient_rawï¼›å¯å¸¦ quantity_* åˆ—ï¼‰
  --usda_dir              USDA FDC è§£å‹ç›®å½•ï¼ˆå« food.csv, food_nutrient.csv, ... å¿…è¦æœ€å°‘ï¼šfood.csvï¼‰
  --fndds_household_csv   å¯é€‰ï¼ŒFNDDS household weight è¡¨
  --A_table_csv           å¯é€‰ï¼ŒA è¡¨ï¼ˆfdc_id, unit, grams_per_unit, ...ï¼‰
  --B_table_csv           å¯é€‰ï¼ŒB è¡¨ï¼ˆterm,synonymï¼‰æˆ–ï¼ˆsrc_term,target_termï¼‰

è¾“å‡ºï¼š
  --out_dir ä¸‹ç”Ÿæˆï¼š
    aligned.parquet                    å¯¹é½ç»“æœï¼ˆæ¯æ¡é…æ–™é€‰å‡ºçš„ fdc_id + æ‰“åˆ†ç»†èŠ‚ï¼‰
    aligned_best.parquet               æ¯æ¡é…æ–™çš„æœ€ä½³é…å¯¹ï¼ˆå»é‡èšåˆï¼‰
    unmatched_topfreq.csv              æœªåŒ¹é…é¡¹çš„é¢‘æ¬¡æ¸…å•ï¼ˆç”¨äºè¡¥è¡¨ï¼‰
    audit_summary.txt                  å®¡è®¡ä¸æŒ‡æ ‡æ¦‚è§ˆ

ä¾èµ–ï¼ˆå¯é€‰ä¼˜é›…é€€åŒ–ï¼‰ï¼š
  pandas, numpy, rapidfuzz, (sentence_transformers, faiss-cpu)

ç”¨æ³•ç¤ºä¾‹ï¼š
python work/recipebench/scripts/rawdataprocess/step2_embedding_generation.py \
    --ingredients_csv work/recipebench/data/4out/ingredients_processed.parquet \
    --usda_dir work/recipebench/data/raw/usda \
    --A_table_csv work/recipebench/data/3out/household_weights_A.csv \
    --out_dir work/recipebench/data/4out/step2 \
    --embed_model sentence-transformers/all-MiniLM-L6-v2 \
    --min_fuzz 72 --topk_lex 80 --topk_embed 120 \
    --fast_precise_mode

python work/recipebench/scripts/rawdataprocess/step2_embedding_generation.py \
  --ingredients_csv work/recipebench/data/5guard/ingredients_labeled.parquet \
  --usda_dir work/recipebench/data/raw/usda \
  --A_table_csv work/recipebench/data/3out/household_weights_A1.csv \
  --out_dir work/recipebench/data/6aligned \
  --embed_model sentence-transformers/all-MiniLM-L6-v2 \
  --min_fuzz 72 --topk_lex 80 --topk_embed 120 --fast_precise_mode \
  --filter_recipe_ids work/recipebench/data/5guard/hightrust_recipe_ids.parquet

ä½œè€…ï¼šYourName (2025-09-14)
"""

from __future__ import annotations
import os
import re
import json
import math
import argparse
from collections import Counter, defaultdict
from typing import List, Tuple, Dict, Optional

import numpy as np
import pandas as pd

# --- ä¾èµ–ä¼˜é›…é™çº§ ---
try:
    from rapidfuzz import fuzz, process as rf_process
    _HAS_RAPIDFUZZ = True
except Exception:
    _HAS_RAPIDFUZZ = False

try:
    from sentence_transformers import SentenceTransformer
    from numpy.linalg import norm
    _HAS_ST = True
except Exception:
    _HAS_ST = False

try:
    import faiss  # å¯é€‰
    _HAS_FAISS = True
except Exception:
    _HAS_FAISS = False

# -------------------- å®ç”¨å‡½æ•° --------------------
TOKEN_SPLIT_RE = re.compile(r"[^a-z0-9]+")
STOP_MODIFIERS = set([
    # å¸¸è§ä¿®é¥°è¯ï¼ˆå¯æ ¹æ®è¯­æ–™æ‰©å……ï¼‰
    "fresh", "organic", "large", "small", "medium", "ripe", "peeled", "seeded",
    "freshly", "ground", "minced", "chopped", "sliced", "diced", "grated", "optional",
    "divided", "to", "taste", "room", "temperature", "unsalted", "salted",
])

PLURAL_RULES = [
    (re.compile(r"(.*)ies$"), r"\1y"),
    (re.compile(r"(.*)oes$"), r"\1o"),
    (re.compile(r"(.*)ses$"), r"\1s"),
    (re.compile(r"(.*)s$"), r"\1"),
]

CATEGORY_KEYWORDS = {
    # ç”¨äº dtype_prior ä¸ default grams ç­–ç•¥
    "spice": ["spice", "seasoning", "powder", "ground"],
    "herb": ["herb", "basil", "cilantro", "parsley", "mint", "rosemary", "thyme"],
    "veg": ["vegetable", "tomato", "onion", "pepper", "carrot", "celery", "broccoli"],
    "meat": ["beef", "pork", "chicken", "turkey", "lamb", "bacon", "ham"],
    "egg": ["egg"],
    "dairy": ["milk", "cheese", "cream", "butter", "yogurt"],
    "liquid": ["sauce", "stock", "broth", "oil", "vinegar", "wine", "water"],
}

DEFAULT_GRAMS_BY_CAT = {
    "spice": 6.0,
    "herb": 5.0,
    "veg": 90.0,
    "meat": 120.0,
    "egg": 50.0,
    "dairy": 30.0,
    "liquid": 20.0,
    "other": 30.0,
}


def normalize_term(term: str) -> str:
    if pd.isna(term):
        return ""
    t = term.strip().lower()
    t = re.sub(r"\(.*?\)", " ", t)  # å»æ‹¬å·å†…å®¹
    t = re.sub(r"\s+", " ", t)
    toks = [w for w in TOKEN_SPLIT_RE.split(t) if w]
    toks2 = []
    for w in toks:
        if w in STOP_MODIFIERS:
            continue
        # å¤æ•°è¿˜åŸ
        w2 = w
        for pat, rep in PLURAL_RULES:
            if pat.fullmatch(w2):
                w2 = pat.sub(rep, w2)
                break
        toks2.append(w2)
    return " ".join(toks2)


def build_syn_map(B_df: Optional[pd.DataFrame]) -> Dict[str, str]:
    syn_map = {}
    if B_df is None or B_df.empty:
        return syn_map
    cols = {c.lower(): c for c in B_df.columns}
    # æ”¯æŒ (term, synonym) æˆ– (src_term, target_term)
    if "term" in cols and "synonym" in cols:
        for _, r in B_df.iterrows():
            a = str(r[cols["term"]]).strip().lower()
            b = str(r[cols["synonym"]]).strip().lower()
            if a and b:
                syn_map[a] = b
    elif "src_term" in cols and "target_term" in cols:
        for _, r in B_df.iterrows():
            a = str(r[cols["src_term"]]).strip().lower()
            b = str(r[cols["target_term"]]).strip().lower()
            if a and b:
                syn_map[a] = b
    return syn_map


def apply_synonym(s: str, syn_map: Dict[str, str]) -> str:
    if not s:
        return s
    if s in syn_map:
        return syn_map[s]
    # token çº§åˆ«æ›¿æ¢ï¼ˆç²—ç•¥ï¼‰
    toks = s.split()
    toks = [syn_map.get(w, w) for w in toks]
    return " ".join(toks)


# -------------------- USDA ç´¢å¼• --------------------

def load_usda_foods(usda_dir: str) -> pd.DataFrame:
    food_csv = os.path.join(usda_dir, "food.csv")
    if not os.path.exists(food_csv):
        raise FileNotFoundError(f"USDA food.csv not found: {food_csv}")
    df = pd.read_csv(food_csv)
    # å…¼å®¹å¸¸è§åˆ—å
    cols = {c.lower(): c for c in df.columns}
    id_col = cols.get("fdc_id", None)
    desc_col = cols.get("description", None)
    cat_col = cols.get("food_category_id", None)
    if not id_col or not desc_col:
        raise ValueError("food.csv must contain fdc_id and description")
    out = df[[id_col, desc_col] + ([cat_col] if cat_col else [])].copy()
    out.columns = ["fdc_id", "description"] + (["food_category_id"] if cat_col else [])
    out["desc_norm"] = out["description"].astype(str).str.lower().map(normalize_term)
    return out


def make_inverted_index(food_df: pd.DataFrame) -> Dict[str, set]:
    inv = defaultdict(set)
    for i, r in food_df.iterrows():
        fid = int(r["fdc_id"]) if not pd.isna(r["fdc_id"]) else None
        if fid is None:
            continue
        for tok in set(r["desc_norm"].split()):
            if tok:
                inv[tok].add(fid)
    return inv


# -------------------- å‘é‡å¬å›ï¼ˆå¯é€‰ï¼‰ --------------------
class EmbedSearcher:
    def __init__(self, model_name: str, food_df: pd.DataFrame, use_faiss: bool = True):
        if not _HAS_ST:
            raise RuntimeError("sentence_transformers not available")
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        self.food_df = food_df
        self.food_texts = food_df["desc_norm"].fillna("").astype(str).tolist()
        
        print(f"ğŸ”„ æ­£åœ¨ä¸º {len(self.food_texts)} ä¸ªé£Ÿç‰©æè¿°ç”Ÿæˆå‘é‡åµŒå…¥...")
        print(f"   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´batch_size
        if len(self.food_texts) > 50000:
            batch_size = 64  # å¤§æ•°æ®é›†ä½¿ç”¨æ›´å°çš„batch
        elif len(self.food_texts) > 10000:
            batch_size = 128
        else:
            batch_size = 256
        
        print(f"   ä½¿ç”¨batch_size: {batch_size}")
        
        # ä½¿ç”¨åŠ¨æ€batch_sizeæ¥å¹³è¡¡é€Ÿåº¦å’Œå†…å­˜
        self.mat = self.model.encode(
            self.food_texts, 
            batch_size=batch_size,
            show_progress_bar=True, 
            convert_to_numpy=True, 
            normalize_embeddings=True
        )
        
        print(f"âœ… å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {self.mat.shape}")
        
        self.use_faiss = use_faiss and _HAS_FAISS
        if self.use_faiss:
            print(f"ğŸ”„ æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
            d = self.mat.shape[1]
            self.index = faiss.IndexFlatIP(d)
            self.index.add(self.mat.astype(np.float32))
            print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆ")
        else:
            self.index = None

    def search(self, query: str, topk: int = 50) -> List[Tuple[int, float]]:
        if not query:
            return []
        qv = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        if self.use_faiss and self.index is not None:
            D, I = self.index.search(qv.astype(np.float32), topk)
            return [(int(ix), float(sc)) for ix, sc in zip(I[0], D[0]) if ix >= 0]
        # é€€åŒ–ç‰ˆï¼šå…¨é‡ä½™å¼¦
        sims = (self.mat @ qv[0]).astype(float)
        idx = np.argpartition(-sims, min(topk, len(sims)-1))[:topk]
        idx = idx[np.argsort(-sims[idx])]
        return [(int(i), float(sims[i])) for i in idx]


# -------------------- é‡æ’åºæ‰“åˆ† --------------------

def safe_ratio(a: str, b: str) -> float:
    if not _HAS_RAPIDFUZZ:
        return 0.0
    return float(fuzz.token_set_ratio(a, b))


def category_prior(desc_norm: str) -> Dict[str, float]:
    s = desc_norm
    scores = {k: 0.0 for k in DEFAULT_GRAMS_BY_CAT}
    for cat, kws in CATEGORY_KEYWORDS.items():
        for kw in kws:
            if kw in s:
                scores[cat] += 1.0
    # å½’ä¸€åŒ–
    tot = sum(scores.values())
    if tot <= 0:
        scores = {k: (1.0 if k == "other" else 0.0) for k in scores}
    else:
        scores = {k: v / tot for k, v in scores.items()}
    return scores


def choose_default_grams(prior: Dict[str, float]) -> float:
    val = 0.0
    for cat, w in prior.items():
        val += w * DEFAULT_GRAMS_BY_CAT.get(cat, DEFAULT_GRAMS_BY_CAT["other"])
    return float(val) if val > 0 else DEFAULT_GRAMS_BY_CAT["other"]


def rerank_score(fuzz_ratio: float, embed_sim: float, dtype_bonus: float, rule_bonus: float,
                 w_fuzz=0.45, w_embed=0.35, w_dtype=0.15, w_rule=0.05) -> float:
    # æ‰€æœ‰é¡¹å‡ä¸º 0..100 æˆ– 0..1 éœ€å½’ä¸€
    f = np.clip(fuzz_ratio / 100.0, 0, 1)
    e = np.clip(embed_sim, 0, 1)
    d = np.clip(dtype_bonus, 0, 1)
    r = np.clip(rule_bonus, 0, 1)
    return float(w_fuzz * f + w_embed * e + w_dtype * d + w_rule * r)


# -------------------- A è¡¨ & FNDDS Household èåˆ --------------------

def load_A_table(A_csv: Optional[str]) -> Optional[pd.DataFrame]:
    if not A_csv or not os.path.exists(A_csv):
        return None
    A = pd.read_csv(A_csv)
    low = {c.lower(): c for c in A.columns}
    need = ["fdc_id", "unit", "grams_per_unit"]
    for n in need:
        if n not in low:
            raise ValueError("A_table_csv ç¼ºå°‘åˆ—ï¼šfdc_id, unit, grams_per_unit")
    A = A[[low["fdc_id"], low["unit"], low["grams_per_unit"]]].copy()
    A.columns = ["fdc_id", "unit", "grams_per_unit"]
    # å¼ºåˆ¶ç±»å‹
    A["fdc_id"] = pd.to_numeric(A["fdc_id"], errors="coerce").astype("Int64")
    A["grams_per_unit"] = pd.to_numeric(A["grams_per_unit"], errors="coerce")
    A = A.dropna(subset=["fdc_id", "unit", "grams_per_unit"]).reset_index(drop=True)
    A["unit_norm"] = A["unit"].astype(str).str.lower().map(normalize_term)
    return A


def load_fndds_household(csv_path: Optional[str]) -> Optional[pd.DataFrame]:
    if not csv_path or not os.path.exists(csv_path):
        return None
    H = pd.read_csv(csv_path)
    low = {c.lower(): c for c in H.columns}
    # å°½é‡å…¼å®¹å¸¸è§åˆ—
    cand_cols = [
        ("fdc_id", "unit", "grams"),
        ("fdc_id", "household_unit", "grams"),
        ("fdc_id", "household_measure", "gram_weight"),
    ]
    match = None
    for cols in cand_cols:
        if all(c in low for c in cols):
            match = cols
            break
    if match is None:
        # ä¸å¼ºåˆ¶
        return None
    cols = [low[c] for c in match]
    H = H[cols].copy()
    H.columns = ["fdc_id", "unit", "grams_per_unit"]
    H["fdc_id"] = pd.to_numeric(H["fdc_id"], errors="coerce").astype("Int64")
    H["grams_per_unit"] = pd.to_numeric(H["grams_per_unit"], errors="coerce")
    H = H.dropna(subset=["fdc_id", "unit", "grams_per_unit"]).reset_index(drop=True)
    H["unit_norm"] = H["unit"].astype(str).str.lower().map(normalize_term)
    return H


# -------------------- ä¸»æµç¨‹ --------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--ingredients_csv", required=True, help="é…æ–™è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒCSVæˆ–Parquetæ ¼å¼ï¼‰")
    ap.add_argument("--usda_dir", required=True)
    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--A_table_csv", default=None)
    ap.add_argument("--B_table_csv", default=None)
    ap.add_argument("--fndds_household_csv", default=None)
    ap.add_argument("--embed_model", default=None, help="sentence-transformers æ¨¡å‹åï¼Œå¯é€‰")
    ap.add_argument("--use_faiss", action="store_true", help="å‘é‡å¬å›ä½¿ç”¨ FAISS")
    ap.add_argument("--fast_mode", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡å‘é‡æ¨¡å‹ï¼Œä»…ä½¿ç”¨æ–‡æœ¬åŒ¹é…")
    ap.add_argument("--fast_precise_mode", action="store_true", help="å¿«é€Ÿç²¾å‡†æ¨¡å¼ï¼šä½¿ç”¨è½»é‡çº§å‘é‡æ¨¡å‹ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡")
    ap.add_argument("--min_fuzz", type=int, default=72)
    ap.add_argument("--topk_lex", type=int, default=60)
    ap.add_argument("--topk_embed", type=int, default=120)
    ap.add_argument("--highfreq_boost", type=int, default=40, help="é«˜é¢‘è¯é¢å¤–æ‰©å®¹ä¸Šé™")
    ap.add_argument("--min_score", type=float, default=0.38, help="æœ€ç»ˆ rerank score ä¸‹é™ï¼ˆ0..1ï¼‰")
    args = ap.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)

    # 1) åŠ è½½é…æ–™
    # æ”¯æŒCSVå’ŒParquetæ ¼å¼
    if args.ingredients_csv.endswith('.parquet'):
        ing = pd.read_parquet(args.ingredients_csv)
    else:
        ing = pd.read_csv(args.ingredients_csv)
    
    ing_cols = {c.lower(): c for c in ing.columns}
    term_col = None
    for k in ["ingredient_norm", "ingredient", "term", "name"]:
        if k in ing_cols:
            term_col = ing_cols[k]
            break
    if term_col is None:
        raise ValueError("ingredients_csv éœ€è¦åŒ…å« ingredient_norm/ingredient/term/name ä¹‹ä¸€")
    ing["term_raw"] = ing[term_col].astype(str)
    ing["term_norm"] = ing["term_raw"].map(normalize_term)

    # è¯é¢‘ç»Ÿè®¡ï¼ˆç”¨äºé«˜é¢‘æ‰©å®¹ï¼‰
    freq = ing["term_norm"].value_counts().to_dict()

    # 2) åŠ è½½ B è¡¨ï¼ˆåŒä¹‰è¯/åˆ«åæ˜ å°„ï¼‰å¹¶åº”ç”¨
    B_df = pd.read_csv(args.B_table_csv) if args.B_table_csv and os.path.exists(args.B_table_csv) else None
    syn_map = build_syn_map(B_df)
    ing["term_norm"] = ing["term_norm"].map(lambda s: apply_synonym(s, syn_map))

    # 3) åŠ è½½ USDA foods å¹¶å»ºç´¢å¼•
    foods = load_usda_foods(args.usda_dir)

    # æŒ‰ B è¡¨ä¹Ÿå¯¹ USDA ç«¯åšä¸€éè½»åº¦ mappingï¼ˆå¯é€‰ï¼‰
    foods["desc_norm_syn"] = foods["desc_norm"].map(lambda s: apply_synonym(s, syn_map))

    inv = make_inverted_index(foods.assign(desc_norm=foods["desc_norm_syn"]))
    
    # ä¿å­˜åŸå§‹foodsçš„fdc_idæ˜ å°„ï¼Œç”¨äºå‘é‡æœç´¢ç»“æœè½¬æ¢
    foods_fdc_mapping = foods["fdc_id"].to_dict()

    # 4) å‘é‡å¬å›ï¼ˆå¯é€‰ï¼‰
    embed_search = None
    if args.fast_mode:
        print("ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡å‘é‡æ¨¡å‹ï¼Œä»…ä½¿ç”¨æ–‡æœ¬åŒ¹é…")
        if args.embed_model:
            print(f"âš ï¸  æ£€æµ‹åˆ° --embed_model å‚æ•°ï¼Œä½†å¿«é€Ÿæ¨¡å¼ä¸‹å°†è¢«å¿½ç•¥")
    elif args.fast_precise_mode:
        print("âš¡ å¿«é€Ÿç²¾å‡†æ¨¡å¼ï¼šä½¿ç”¨è½»é‡çº§å‘é‡æ¨¡å‹")
        if not _HAS_ST:
            print("[WARN] sentence_transformers ä¸å¯ç”¨ï¼Œå°†é™çº§ä¸ºå¿«é€Ÿæ¨¡å¼")
        else:
            # ä½¿ç”¨æ›´å°çš„æ¨¡å‹å’Œä¼˜åŒ–çš„å‚æ•°
            fast_model = "sentence-transformers/all-MiniLM-L6-v2"
            try:
                print(f"ğŸ”„ åŠ è½½è½»é‡çº§æ¨¡å‹: {fast_model}")
                embed_search = EmbedSearcher(fast_model, foods.assign(desc_norm=foods["desc_norm_syn"]), use_faiss=False)  # ä¸ä½¿ç”¨FAISSä»¥èŠ‚çœå†…å­˜
                print("âœ… å¿«é€Ÿç²¾å‡†æ¨¡å¼å‡†å¤‡å®Œæˆ")
            except Exception as e:
                print(f"[WARN] å¿«é€Ÿç²¾å‡†æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
                print("[WARN] å°†é™çº§ä¸ºå¿«é€Ÿæ¨¡å¼")
                embed_search = None
    elif args.embed_model:
        if not _HAS_ST:
            print("[WARN] sentence_transformers ä¸å¯ç”¨ï¼Œè·³è¿‡å‘é‡å¬å›")
        else:
            try:
                embed_search = EmbedSearcher(args.embed_model, foods.assign(desc_norm=foods["desc_norm_syn"]), use_faiss=args.use_faiss)
            except Exception as e:
                print(f"[WARN] å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("[WARN] å°†è·³è¿‡å‘é‡å¬å›ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–æ–¹æ³•")
                embed_search = None
    else:
        print("â„¹ï¸  æœªæŒ‡å®šå‘é‡æ¨¡å‹ï¼Œå°†ä»…ä½¿ç”¨æ–‡æœ¬åŒ¹é…")

    # 5) A è¡¨ + FNDDS householdï¼ˆèåˆï¼‰
    A = load_A_table(args.A_table_csv)
    H = load_fndds_household(args.fndds_household_csv)
    G = None
    if A is not None and H is not None:
        G = pd.concat([A, H], ignore_index=True)
    elif A is not None:
        G = A.copy()
    elif H is not None:
        G = H.copy()

    # 6) åŒ¹é… - å”¯ä¸€æœ¯è¯­åŒ¹é…åå›å¡«
    # é¢„æ„å»ºä¾¿æ·æ˜ å°„
    fdc_to_desc = foods.set_index("fdc_id")["desc_norm_syn"].to_dict()

    # 1) åªå–å”¯ä¸€æœ¯è¯­ï¼Œå¹¶ç»Ÿè®¡é¢‘æ¬¡ï¼ˆç”¨äºè‡ªé€‚åº” topkï¼‰
    term_counts = ing["term_norm"].value_counts()
    terms_unique = term_counts.index.tolist()

    print(f"ğŸ”„ å¼€å§‹åŒ¹é…å”¯ä¸€æœ¯è¯­ {len(terms_unique)} ä¸ªï¼ˆåŸå§‹è¡Œ {len(ing)} ï¼‰...")

    # 2) ï¼ˆå¯é€‰ï¼‰ä¸ºå”¯ä¸€æœ¯è¯­ä¸€æ¬¡æ€§ç”ŸæˆæŸ¥è¯¢å‘é‡å¹¶ç¼“å­˜
    qvec_cache = {}
    if embed_search is not None:
        # embed_search.model å·²åŠ è½½ï¼›ç”¨ç›¸åŒ normalize è®¾ç½®
        # æ³¨æ„ï¼šå¤§æ‰¹é‡ç¼–ç å¯åˆ†æ‰¹
        from math import ceil
        BATCH = 4096
        for i in range(0, len(terms_unique), BATCH):
            batch = terms_unique[i:i+BATCH]
            vecs = embed_search.model.encode(
                batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False
            )
            for t, v in zip(batch, vecs):
                qvec_cache[t] = v

    best_by_term = {}      # term_norm -> dict(fdc_id, score, fuzz, embed_sim, ...)
    detail_rows = []       # å¯é€‰ï¼šä¿å­˜å€™é€‰ç»†èŠ‚ï¼ˆç”¨äºå®¡è®¡ï¼‰

    from tqdm import tqdm
    for term in tqdm(terms_unique, desc="åŒ¹é…å”¯ä¸€æœ¯è¯­"):
        # è‡ªé€‚åº” topkï¼ˆé«˜é¢‘æ‰æ‰©å®¹ï¼‰
        local_topk_lex = min(args.topk_lex + (args.highfreq_boost if term_counts[term] > 100 else 0), 120)

        # ---- è¯å…¸å€’æ’å¬å› ----
        toks = set(term.split())
        lex_pool = set()
        for t in toks:
            # è·³è¿‡é«˜ DF çš„"æ³›è¯"tokenï¼Œé¿å…å€™é€‰çˆ†ç‚¸
            if t in {"powder","oil","sauce","ground","fresh"}:
                continue
            if t in inv:
                # é™åˆ¶æ¯ä¸ª token å¸¦æ¥çš„å€™é€‰ä¸Šé™ï¼Œé¿å…è¶…å¤§ DF
                cands = list(inv[t])
                if len(cands) > 200:
                    cands = cands[:200]
                lex_pool |= set(cands)
        lex_pool = list(lex_pool)[:max(local_topk_lex, 1)]

        # ---- å‘é‡å¬å›ï¼ˆå¯é€‰ï¼‰ ----
        embed_cands = []
        embed_sims = {}
        if embed_search is not None:
            # ç”¨ç¼“å­˜çš„æŸ¥è¯¢å‘é‡ï¼ˆè‹¥æ— åˆ™å³æ—¶ç¼–ç ï¼‰
            qv = qvec_cache.get(term, None)
            if qv is None:
                qv = embed_search.model.encode([term], convert_to_numpy=True, normalize_embeddings=True)[0]
                qvec_cache[term] = qv
            if getattr(embed_search, "index", None) is not None:
                # FAISS è·¯å¾„
                D, I = embed_search.index.search(np.asarray([qv], dtype=np.float32), args.topk_embed)
                hits = [(int(ix), float(sc)) for ix, sc in zip(I[0], D[0]) if ix >= 0]
                embed_cands = [foods_fdc_mapping[i] for (i, _sc) in hits if i in foods_fdc_mapping]
                embed_sims = {foods_fdc_mapping[i]: float(sc) for (i, sc) in hits if i in foods_fdc_mapping}
            else:
                # é€€åŒ–å…¨é‡ï¼ˆå·²åœ¨ embed_search.search å®ç°ï¼Œå¯ç›´æ¥è°ƒç”¨ï¼‰
                hits = embed_search.search(term, topk=args.topk_embed)
                embed_cands = [foods_fdc_mapping[i] for (i, _sc) in hits if i in foods_fdc_mapping]
                embed_sims = {foods_fdc_mapping[i]: float(sc) for (i, sc) in hits if i in foods_fdc_mapping}

        pool = set(lex_pool) | set(embed_cands)
        if not pool:
            best_by_term[term] = None
            continue

        # ---- æ‰“åˆ† ----
        best = None
        for fdc in pool:
            desc = fdc_to_desc.get(fdc, "")
            if not desc:
                continue
            fr = safe_ratio(term, desc)  # 0..100
            if fr < args.min_fuzz and fdc not in embed_sims:
                continue
            es = float(embed_sims.get(fdc, 0.0))
            prior = category_prior(desc)
            dtype_bonus = max(prior.values())
            rule_bonus = 1.0 if any(k in term for k in ["egg","chicken","tomato","onion","pepper","oil"]) and \
                               any(k in desc for k in ["egg","chicken","tomato","onion","pepper","oil"]) else 0.0
            sc = rerank_score(fr, es, dtype_bonus, rule_bonus)
            if (best is None or sc > best["score"]):
                best = {"fdc_id": int(fdc), "score": sc, "fuzz": fr, "embed_sim": es,
                        "dtype_bonus": dtype_bonus, "rule_bonus": rule_bonus}
        if best and best["score"] >= args.min_score:
            best_by_term[term] = best
        else:
            best_by_term[term] = None

    # æŠŠ term çº§åˆ«çš„åŒ¹é…ç»“æœå¹¿æ’­å›åŸå§‹è¡Œ
    best_df = (pd.Series(best_by_term, name="best")
                 .to_frame()
                 .reset_index().rename(columns={"index":"term_norm"}))
    # æ‹†å¼€ best å­—å…¸
    def expand_best_dict(x, _np=np, _pd=pd):
        if isinstance(x, dict):
            return _pd.Series(x)
        return _pd.Series({
            "fdc_id": _np.nan, "score": _np.nan, "fuzz": _np.nan, "embed_sim": _np.nan,
            "dtype_bonus": _np.nan, "rule_bonus": _np.nan
        })
    
    best_df = pd.concat([
        best_df[["term_norm"]],
        best_df["best"].apply(expand_best_dict)
    ], axis=1)

    # ç»Ÿä¸€ç¼ºå¤±å€¼ & æ•°æ®ç±»å‹
    best_df["fdc_id"] = pd.to_numeric(best_df["fdc_id"], errors="coerce").astype("Int64")
    num_cols = ["score","fuzz","embed_sim","dtype_bonus","rule_bonus"]
    for c in num_cols:
        best_df[c] = pd.to_numeric(best_df[c], errors="coerce")

    aligned = ing.merge(best_df, on="term_norm", how="left")

    # 8) å…‹é‡å›å¡«ï¼ˆA è¡¨ / household / åŠ¨æ€é»˜è®¤ï¼‰
    if G is not None:
        # å…ˆå¯¹ unit åšè§„èŒƒåŒ–åŒ¹é…ï¼ˆè‹¥é…æ–™è¡¨å« unit åˆ—ï¼‰
        unit_col = None
        for k in ["unit_std", "unit", "unit_norm", "quantity_unit", "qty_unit"]:
            if k in ing_cols:
                unit_col = ing_cols[k]
                break
        if unit_col is not None:
            aligned["unit_norm"] = aligned[unit_col].astype(str).str.lower().map(normalize_term)
            G_key = G[["fdc_id", "unit_norm", "grams_per_unit"]].dropna().copy()
            aligned = aligned.merge(G_key, on=["fdc_id", "unit_norm"], how="left", suffixes=("", "_fromG"))
        else:
            aligned["grams_per_unit"] = np.nan

    # åŠ¨æ€é»˜è®¤å…‹é‡ï¼šæŒ‰åŒ¹é…æè¿°ç±»åˆ« prior
    # è‹¥ä¸èƒ½ä» G/A è¡¨é‡Œæ‹¿åˆ°å•ä½å…‹é‡ï¼Œåˆ™ç»™å®šä¸€ä¸ª default_grams_by_cat
    def _default_g(row, _np=np, _pd=pd, _fdc_to_desc=fdc_to_desc, _category_prior=category_prior, _choose_default_grams=choose_default_grams):
        if not _pd.isna(row.get("grams_per_unit", _np.nan)):
            return row["grams_per_unit"]
        desc = _fdc_to_desc.get(row["fdc_id"], "") if not _pd.isna(row.get("fdc_id", _np.nan)) else ""
        prior = _category_prior(desc)
        return _choose_default_grams(prior)

    aligned["grams_per_unit_fill"] = aligned.apply(_default_g, axis=1)

    # 9) è¾“å‡º
    aligned_path = os.path.join(args.out_dir, "aligned.parquet")
    best_path = os.path.join(args.out_dir, "aligned_best.parquet")

    # ä¿ç•™å…³é”®ä¿¡æ¯
    keep_cols = [c for c in aligned.columns if c not in {"row_id"}]
    aligned[keep_cols].to_parquet(aligned_path, index=False)

    # å¯¹æ¯æ¡åŸå§‹é…æ–™åªä¿ç•™æœ€ä½³åŒ¹é…è¡Œ
    aligned_best = aligned.dropna(subset=["fdc_id"])\
                         .sort_values(["term_norm", "score"], ascending=[True, False])\
                         .groupby("term_norm", as_index=False).first()
    aligned_best.to_parquet(best_path, index=False)

    # 10) æœªåŒ¹é…é¡¹ç»Ÿè®¡ï¼ˆerror-awareï¼‰
    unmatched = aligned[aligned["fdc_id"].isna()]["term_norm"].value_counts().reset_index()
    unmatched.columns = ["term", "count"]
    unmatched_path = os.path.join(args.out_dir, "unmatched_topfreq.csv")
    unmatched.to_csv(unmatched_path, index=False)

    # 11) å®¡è®¡ quick report
    tot = len(ing)
    hit = len(aligned[aligned["fdc_id"].notna()])
    hit_rate = 100.0 * hit / max(tot, 1)

    avg_fuzz = aligned["fuzz"].mean() if not aligned.empty else 0.0
    avg_embed = aligned["embed_sim"].mean() if not aligned.empty else 0.0

    audit = {
        "ingredients": tot,
        "matched": int(hit),
        "hit_rate_percent": round(hit_rate, 2),
        "min_fuzz": args.min_fuzz,
        "min_score": args.min_score,
        "avg_fuzz_in_candidates": round(float(avg_fuzz), 2),
        "avg_embed_in_candidates": round(float(avg_embed), 4),
        "use_embeddings": bool(embed_search is not None),
        "use_faiss": bool(embed_search is not None and args.use_faiss and _HAS_FAISS),
        "A_table_loaded": bool(A is not None),
        "household_loaded": bool(H is not None),
    }

    with open(os.path.join(args.out_dir, "audit_summary.txt"), "w", encoding="utf-8") as f:
        f.write(json.dumps(audit, ensure_ascii=False, indent=2))

    print("==== step2 å¯¹é½å®Œæ¯• ====")
    print(json.dumps(audit, ensure_ascii=False, indent=2))
    print(f"ä¿å­˜ï¼š{aligned_path}\nä¿å­˜ï¼š{best_path}\næœªåŒ¹é…é¢‘æ¬¡ï¼š{unmatched_path}")


if __name__ == "__main__":
    main()
