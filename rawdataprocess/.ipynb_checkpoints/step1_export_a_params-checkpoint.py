# -*- coding: utf-8 -*-
"""
export_a_params.py â€” Aè¡¨å‚æ•°å¯¼å‡ºè„šæœ¬

åŠŸèƒ½æ¦‚è¿°ï¼š
    å°†Aè¡¨çš„å¤–é”®ä¼˜åŠ¿è½¬ä¸ºå¯å¤ç”¨çš„lookupè¡¨ï¼Œä¸ºåç»­Stepå®ç°ingredientå’Œfdc_idå¯¹é½æ—¶
    æä¾›ç²¾å‡†çš„å…‹é‡æ¢ç®—å‚æ•°ã€‚æœ¬è„šæœ¬ç‹¬ç«‹äºStep1ä¸»æµç¨‹ï¼Œä¸æ±¡æŸ“æ ¸å¿ƒå¤„ç†é€»è¾‘ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. fdcçº§ä½“ç§¯å¯†åº¦ï¼ˆg/mlï¼‰ï¼šæŒ‰fdc_idèšåˆä½“ç§¯å•ä½çš„å¯†åº¦æ•°æ®
    2. fdcÃ—unitçº§ä»¶æ•°å…‹é‡ï¼ˆg/ä»¶ï¼‰ï¼šæŒ‰(fdc_id, unit)èšåˆä»¶æ•°å•ä½çš„é‡é‡æ•°æ®
    3. å…¨å±€ä¸­ä½æ•°ï¼ˆå…œåº•ï¼‰ï¼šæŒ‰å•ä½èšåˆçš„å…¨å±€è½¬æ¢ç³»æ•°
    4. ç±»åˆ«/Tokenèšåˆï¼ˆæ¬¡ä¼˜å›é€€ï¼‰ï¼šæŒ‰food_category_idå’Œtokenèšåˆçš„è½¬æ¢å‚æ•°

è¾“å‡ºæ–‡ä»¶ï¼š
    - A_fdc_volume_density.parquet: fdcçº§ä½“ç§¯å¯†åº¦è¡¨
    - A_fdc_piece_weight.parquet: fdcÃ—unitçº§ä»¶æ•°å…‹é‡è¡¨
    - A_unit_global_median.parquet: å…¨å±€ä¸­ä½æ•°è¡¨
    - A_cat_token_aggregates.parquet: ç±»åˆ«/Tokenèšåˆè¡¨

ä½¿ç”¨æ–¹æ³•ï¼š
    python work/recipebench/scripts/rawdataprocess/step1_export_a_params.py \
      --a_table_path work/recipebench/data/3out/household_weights_A.csv \
      --food_path /path/to/food_processed.parquet \
      --out_dir /path/to/output/params

è®¾è®¡è€ƒè™‘ï¼š
    - ä½¿ç”¨winsorizeå»æå€¼å¤„ç†ï¼Œæé«˜æ•°æ®è´¨é‡
    - æ”¯æŒä½“ç§¯â†’å¯†åº¦å’Œä»¶æ•°â†’é‡é‡çš„åˆ†å±‚æ˜ å°„ç­–ç•¥
    - æä¾›fdc_idã€categoryã€tokenä¸‰ä¸ªå±‚çº§çš„å›é€€æœºåˆ¶
    - è¾“å‡ºparquetæ ¼å¼ï¼Œä¾¿äºåç»­é«˜æ•ˆæŸ¥è¯¢
"""

import os
import argparse
import pandas as pd
import numpy as np
import warnings
from tqdm import tqdm
from common_utils import tokenize

# =============================================================================
# å•ä½æ˜ å°„é…ç½®
# =============================================================================

# ä½“ç§¯å•ä½åˆ°æ¯«å‡çš„è½¬æ¢ç³»æ•°
ML_PER = {
    'tsp': 4.92892,        # èŒ¶åŒ™åˆ°æ¯«å‡
    'tbsp': 14.7868,       # æ±¤åŒ™åˆ°æ¯«å‡
    'cup': 236.588,        # æ¯åˆ°æ¯«å‡
    'fl_oz': 29.5735,      # æ¶²ä½“ç›å¸åˆ°æ¯«å‡
    'pt': 473.176,         # å“è„±åˆ°æ¯«å‡
    'qt': 946.353,         # å¤¸è„±åˆ°æ¯«å‡
    'l': 1000,             # å‡åˆ°æ¯«å‡
    'ml': 1,               # æ¯«å‡åˆ°æ¯«å‡ï¼ˆåŸºå‡†ï¼‰
    'gallon': 3785.41,     # åŠ ä»‘åˆ°æ¯«å‡
    'pint': 473.176,       # å“è„±åˆ°æ¯«å‡
    'quart': 946.353,      # å¤¸è„±åˆ°æ¯«å‡
    'liter': 1000,         # å‡åˆ°æ¯«å‡
    'milliliter': 1,       # æ¯«å‡åˆ°æ¯«å‡
    'fluid_ounce': 29.5735, # æ¶²ä½“ç›å¸åˆ°æ¯«å‡
    'teaspoon': 4.92892,   # èŒ¶åŒ™åˆ°æ¯«å‡
    'tablespoon': 14.7868, # æ±¤åŒ™åˆ°æ¯«å‡
}

# ä»¶æ•°å•ä½é›†åˆ
PIECE_UNITS = {
    'piece', 'slice', 'sheet', 'clove', 'stick', 'can', 'package', 'packet',
    'serving', 'head', 'jar', 'bag', 'bunch', 'sprig', 'egg', 'drumstick', 
    'thigh', 'steak', 'stalk', 'link', 'banana', 'spear', 'bottle', 'box',
    'carton', 'container', 'bar', 'fillet', 'breast', 'wing', 'rib'
}

# =============================================================================
# å¯¼å‡ºå‡½æ•°å®ç°
# =============================================================================

def export_a_fdc_volume_density(a_df, ml_map, out_path):
    """
    å¯¼å‡ºfdcçº§ä½“ç§¯å¯†åº¦ï¼ˆg/mlï¼‰
    
    åŠŸèƒ½è¯´æ˜ï¼š
        1. ç­›é€‰ä½“ç§¯å•ä½è®°å½•
        2. è®¡ç®—å¯†åº¦ = grams_per_unit / ml_per_unit
        3. æŒ‰fdc_idè¿›è¡Œwinsorizeå»æå€¼å¤„ç†
        4. å–ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆå¯†åº¦å€¼
        5. è®°å½•æ ·æœ¬æ•°é‡ç”¨äºè´¨é‡è¯„ä¼°
    
    å‚æ•°ï¼š
        a_df (pd.DataFrame): Aè¡¨æ•°æ®
        ml_map (dict): ä½“ç§¯å•ä½åˆ°æ¯«å‡çš„è½¬æ¢æ˜ å°„
        out_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¾“å‡ºåˆ—ï¼š
        - fdc_id: é£Ÿç‰©ID
        - density_g_per_ml: å¯†åº¦ï¼ˆå…‹/æ¯«å‡ï¼‰
        - n: æ ·æœ¬æ•°é‡
    """
    print("   æ­£åœ¨å¯¼å‡ºfdcçº§ä½“ç§¯å¯†åº¦...")
    
    # ä»…å–ä½“ç§¯å•ä½è®°å½•
    v = a_df[a_df["unit_std"].isin(ml_map.keys())].copy()
    if v.empty:
        print("   âš ï¸  æ²¡æœ‰ä½“ç§¯å•ä½æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    # è®¡ç®—æ¯«å‡è½¬æ¢ç³»æ•°
    v["ml_per_unit"] = v["unit_std"].map(ml_map).astype("float64")
    v = v[v["ml_per_unit"] > 0]
    
    # è®¡ç®—å¯†åº¦
    v["density_g_per_ml"] = v["grams_per_unit"].astype("float64") / v["ml_per_unit"]
    
    # è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆå¯†åº¦èŒƒå›´ï¼š0.1-3.0 g/mlï¼‰
    v = v[(v["density_g_per_ml"] >= 0.1) & (v["density_g_per_ml"] <= 3.0)]
    
    if v.empty:
        print("   âš ï¸  è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆå¯†åº¦æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    # winsorize per fdc_idï¼ˆ2.5%-97.5%åˆ†ä½æ•°ï¼‰
    def _winsorize_median(x):
        if len(x) < 3:
            return x.median()
        q025, q975 = x.quantile([0.025, 0.975])
        return x.clip(lower=q025, upper=q975).median()
    
    g = (v.groupby("fdc_id")["density_g_per_ml"]
           .apply(_winsorize_median)
           .reset_index(name="density_g_per_ml"))
    
    # è®°å½•æ ·æœ¬æ•°é‡
    n = v.groupby("fdc_id").size().reset_index(name="n")
    out = g.merge(n, on="fdc_id", how="left")
    
    # ä¿å­˜ç»“æœ
    out.to_parquet(out_path, index=False)
    print(f"   âœ… å¯¼å‡ºå®Œæˆ: {len(out)} ä¸ªfdc_idï¼Œä¿å­˜åˆ° {out_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"   ğŸ“Š å¯†åº¦ç»Ÿè®¡: min={out['density_g_per_ml'].min():.3f}, "
          f"median={out['density_g_per_ml'].median():.3f}, "
          f"max={out['density_g_per_ml'].max():.3f}")

def export_a_fdc_piece_weight(a_df, piece_units, out_path):
    """
    å¯¼å‡ºfdcÃ—unitçº§ä»¶æ•°å…‹é‡ï¼ˆg/ä»¶ï¼‰
    
    åŠŸèƒ½è¯´æ˜ï¼š
        1. ç­›é€‰ä»¶æ•°å•ä½è®°å½•
        2. æŒ‰(fdc_id, unit)è¿›è¡Œwinsorizeå»æå€¼å¤„ç†
        3. å–ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆä»¶é‡å€¼
        4. è®°å½•æ ·æœ¬æ•°é‡ç”¨äºè´¨é‡è¯„ä¼°
    
    å‚æ•°ï¼š
        a_df (pd.DataFrame): Aè¡¨æ•°æ®
        piece_units (set): ä»¶æ•°å•ä½é›†åˆ
        out_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¾“å‡ºåˆ—ï¼š
        - fdc_id: é£Ÿç‰©ID
        - unit_std: æ ‡å‡†åŒ–å•ä½
        - grams_per_unit_clean: æ¯ä»¶å…‹é‡ï¼ˆå»æå€¼åï¼‰
        - n: æ ·æœ¬æ•°é‡
    """
    print("   æ­£åœ¨å¯¼å‡ºfdcÃ—unitçº§ä»¶æ•°å…‹é‡...")
    
    # ç­›é€‰ä»¶æ•°å•ä½è®°å½•
    p = a_df[a_df["unit_std"].isin(piece_units)].copy()
    if p.empty:
        print("   âš ï¸  æ²¡æœ‰ä»¶æ•°å•ä½æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    # è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆä»¶é‡èŒƒå›´ï¼š0.1-1000gï¼‰
    p = p[(p["grams_per_unit"] >= 0.1) & (p["grams_per_unit"] <= 1000)]
    
    if p.empty:
        print("   âš ï¸  è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆä»¶é‡æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    # winsorize per (fdc_id, unit)
    def _winsorize_median(x):
        if len(x) < 3:
            return x.median()
        q025, q975 = x.quantile([0.025, 0.975])
        return x.clip(lower=q025, upper=q975).median()
    
    g = (p.groupby(["fdc_id", "unit_std"])["grams_per_unit"]
           .apply(_winsorize_median)
           .reset_index(name="grams_per_unit_clean"))
    
    # è®°å½•æ ·æœ¬æ•°é‡
    n = p.groupby(["fdc_id", "unit_std"]).size().reset_index(name="n")
    out = g.merge(n, on=["fdc_id", "unit_std"], how="left")
    
    # ä¿å­˜ç»“æœ
    out.to_parquet(out_path, index=False)
    print(f"   âœ… å¯¼å‡ºå®Œæˆ: {len(out)} ä¸ª(fdc_id, unit)å¯¹ï¼Œä¿å­˜åˆ° {out_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"   ğŸ“Š ä»¶é‡ç»Ÿè®¡: min={out['grams_per_unit_clean'].min():.1f}g, "
          f"median={out['grams_per_unit_clean'].median():.1f}g, "
          f"max={out['grams_per_unit_clean'].max():.1f}g")

def export_a_unit_global_median(a_df, out_path):
    """
    å¯¼å‡ºå…¨å±€ä¸­ä½æ•°ï¼ˆå…œåº•ï¼‰
    
    åŠŸèƒ½è¯´æ˜ï¼š
        1. æŒ‰å•ä½èšåˆæ‰€æœ‰è®°å½•
        2. è®¡ç®—æ¯ä¸ªå•ä½çš„ä¸­ä½æ•°è½¬æ¢ç³»æ•°
        3. è®°å½•æ ·æœ¬æ•°é‡ç”¨äºè´¨é‡è¯„ä¼°
        4. ä½œä¸ºæœ€ç»ˆå…œåº•æ–¹æ¡ˆä½¿ç”¨
    
    å‚æ•°ï¼š
        a_df (pd.DataFrame): Aè¡¨æ•°æ®
        out_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¾“å‡ºåˆ—ï¼š
        - unit_std: æ ‡å‡†åŒ–å•ä½
        - grams_per_unit_global_median: å…¨å±€ä¸­ä½æ•°è½¬æ¢ç³»æ•°
        - n: æ ·æœ¬æ•°é‡
    """
    print("   æ­£åœ¨å¯¼å‡ºå…¨å±€ä¸­ä½æ•°...")
    
    # è¿‡æ»¤å¼‚å¸¸å€¼
    a_clean = a_df[(a_df["grams_per_unit"] >= 0.1) & (a_df["grams_per_unit"] <= 10000)]
    
    if a_clean.empty:
        print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    # æŒ‰å•ä½èšåˆä¸­ä½æ•°
    g = (a_clean.groupby("unit_std")["grams_per_unit"]
           .median().reset_index()
           .rename(columns={"grams_per_unit": "grams_per_unit_global_median"}))
    
    # è®°å½•æ ·æœ¬æ•°é‡
    n = a_clean.groupby("unit_std").size().reset_index(name="n")
    out = g.merge(n, on="unit_std", how="left")
    
    # ä¿å­˜ç»“æœ
    out.to_parquet(out_path, index=False)
    print(f"   âœ… å¯¼å‡ºå®Œæˆ: {len(out)} ä¸ªå•ä½ï¼Œä¿å­˜åˆ° {out_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"   ğŸ“Š å•ä½è¦†ç›–: {len(out)} ç§å•ä½ï¼Œæ€»æ ·æœ¬æ•° {out['n'].sum():,}")

def export_a_cat_token_aggregates(a_df, food_df, ml_map, piece_units, out_path):
    """
    å¯¼å‡ºç±»åˆ«/Tokenèšåˆï¼ˆæ¬¡ä¼˜å›é€€ï¼‰
    
    åŠŸèƒ½è¯´æ˜ï¼š
        1. å°†Aè¡¨è¿æ¥åˆ°USDA foodè¡¨è·å–categoryå’Œdescriptionä¿¡æ¯
        2. æŒ‰food_category_idèšåˆä½“ç§¯å¯†åº¦å’Œä»¶æ•°é‡é‡
        3. ä»description_normæå–tokenï¼ŒæŒ‰tokenèšåˆ
        4. ç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒå¤šå±‚çº§å›é€€æŸ¥è¯¢
    
    å‚æ•°ï¼š
        a_df (pd.DataFrame): Aè¡¨æ•°æ®
        food_df (pd.DataFrame): é£Ÿç‰©ä¿¡æ¯è¡¨
        ml_map (dict): ä½“ç§¯å•ä½åˆ°æ¯«å‡çš„è½¬æ¢æ˜ å°„
        piece_units (set): ä»¶æ•°å•ä½é›†åˆ
        out_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    è¾“å‡ºåˆ—ï¼š
        - key: èšåˆé”®ï¼ˆcategory_idæˆ–tokenï¼‰
        - key_type: é”®ç±»å‹ï¼ˆ'cat'æˆ–'token'ï¼‰
        - unit_std: æ ‡å‡†åŒ–å•ä½
        - value: è½¬æ¢å€¼ï¼ˆå¯†åº¦æˆ–é‡é‡ï¼‰
        - n: æ ·æœ¬æ•°é‡
    """
    print("   æ­£åœ¨å¯¼å‡ºç±»åˆ«/Tokenèšåˆ...")
    
    # è¿æ¥Aè¡¨å’Œfoodè¡¨
    m = a_df.merge(
        food_df[["fdc_id", "food_category_id", "description_norm"]], 
        on="fdc_id", how="inner"
    ).dropna(subset=["food_category_id"])
    
    if m.empty:
        print("   âš ï¸  Aè¡¨ä¸foodè¡¨è¿æ¥åæ— æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
        return
    
    results = []
    
    # 1. ä½“ç§¯å•ä½ï¼šè½¬å¯†åº¦
    v = m[m["unit_std"].isin(ml_map.keys())].copy()
    if not v.empty:
        v["density_g_per_ml"] = v["grams_per_unit"].astype("float64") / v["unit_std"].map(ml_map)
        v = v[(v["density_g_per_ml"] >= 0.1) & (v["density_g_per_ml"] <= 3.0)]
        
        if not v.empty:
            # æŒ‰categoryèšåˆ
            v_cat = (v.groupby(["food_category_id", "unit_std"])["density_g_per_ml"]
                       .median().reset_index())
            v_cat["key_type"] = "cat"
            v_cat["key"] = v_cat["food_category_id"]
            v_cat["value"] = v_cat["density_g_per_ml"]
            v_cat = v_cat[["key", "key_type", "unit_std", "value"]]
            
            # è®°å½•æ ·æœ¬æ•°é‡
            v_cat_n = v.groupby(["food_category_id", "unit_std"]).size().reset_index(name="n")
            v_cat = v_cat.merge(v_cat_n, left_on=["key", "unit_std"], 
                               right_on=["food_category_id", "unit_std"], how="left")
            v_cat = v_cat[["key", "key_type", "unit_std", "value", "n"]]
            
            results.append(v_cat)
            print(f"   ğŸ“Š ä½“ç§¯å•ä½categoryèšåˆ: {len(v_cat)} ä¸ª(category, unit)å¯¹")
    
    # 2. ä»¶æ•°å•ä½ï¼šç›´æ¥å…‹é‡
    p = m[m["unit_std"].isin(piece_units)].copy()
    if not p.empty:
        p = p[(p["grams_per_unit"] >= 0.1) & (p["grams_per_unit"] <= 1000)]
        
        if not p.empty:
            # æŒ‰categoryèšåˆ
            p_cat = (p.groupby(["food_category_id", "unit_std"])["grams_per_unit"]
                       .median().reset_index())
            p_cat["key_type"] = "cat"
            p_cat["key"] = p_cat["food_category_id"]
            p_cat["value"] = p_cat["grams_per_unit"]
            p_cat = p_cat[["key", "key_type", "unit_std", "value"]]
            
            # è®°å½•æ ·æœ¬æ•°é‡
            p_cat_n = p.groupby(["food_category_id", "unit_std"]).size().reset_index(name="n")
            p_cat = p_cat.merge(p_cat_n, left_on=["key", "unit_std"], 
                               right_on=["food_category_id", "unit_std"], how="left")
            p_cat = p_cat[["key", "key_type", "unit_std", "value", "n"]]
            
            results.append(p_cat)
            print(f"   ğŸ“Š ä»¶æ•°å•ä½categoryèšåˆ: {len(p_cat)} ä¸ª(category, unit)å¯¹")
    
    # 3. Tokenèšåˆ
    print("   æ­£åœ¨æå–tokenå¹¶èšåˆ...")
    
    # ä»description_normæå–token
    def extract_tokens(s):
        if not isinstance(s, str):
            return []
        return [t for t in tokenize(s) if len(t) >= 3]
    
    m["tokens"] = m["description_norm"].map(extract_tokens)
    m_expanded = m.explode("tokens").dropna(subset=["tokens"])
    
    if not m_expanded.empty:
        # ä½“ç§¯å•ä½tokenèšåˆ
        if not v.empty:
            v_tok = v.merge(m_expanded[["fdc_id", "tokens"]], on="fdc_id", how="inner")
            if not v_tok.empty:
                v_tok_agg = (v_tok.groupby(["tokens", "unit_std"])["density_g_per_ml"]
                               .median().reset_index())
                v_tok_agg["key_type"] = "token"
                v_tok_agg["key"] = v_tok_agg["tokens"]
                v_tok_agg["value"] = v_tok_agg["density_g_per_ml"]
                v_tok_agg = v_tok_agg[["key", "key_type", "unit_std", "value"]]
                
                # è®°å½•æ ·æœ¬æ•°é‡
                v_tok_n = v_tok.groupby(["tokens", "unit_std"]).size().reset_index(name="n")
                v_tok_agg = v_tok_agg.merge(v_tok_n, left_on=["key", "unit_std"], 
                                           right_on=["tokens", "unit_std"], how="left")
                v_tok_agg = v_tok_agg[["key", "key_type", "unit_std", "value", "n"]]
                
                results.append(v_tok_agg)
                print(f"   ğŸ“Š ä½“ç§¯å•ä½tokenèšåˆ: {len(v_tok_agg)} ä¸ª(token, unit)å¯¹")
        
        # ä»¶æ•°å•ä½tokenèšåˆ
        if not p.empty:
            p_tok = p.merge(m_expanded[["fdc_id", "tokens"]], on="fdc_id", how="inner")
            if not p_tok.empty:
                p_tok_agg = (p_tok.groupby(["tokens", "unit_std"])["grams_per_unit"]
                               .median().reset_index())
                p_tok_agg["key_type"] = "token"
                p_tok_agg["key"] = p_tok_agg["tokens"]
                p_tok_agg["value"] = p_tok_agg["grams_per_unit"]
                p_tok_agg = p_tok_agg[["key", "key_type", "unit_std", "value"]]
                
                # è®°å½•æ ·æœ¬æ•°é‡
                p_tok_n = p_tok.groupby(["tokens", "unit_std"]).size().reset_index(name="n")
                p_tok_agg = p_tok_agg.merge(p_tok_n, left_on=["key", "unit_std"], 
                                           right_on=["tokens", "unit_std"], how="left")
                p_tok_agg = p_tok_agg[["key", "key_type", "unit_std", "value", "n"]]
                
                results.append(p_tok_agg)
                print(f"   ğŸ“Š ä»¶æ•°å•ä½tokenèšåˆ: {len(p_tok_agg)} ä¸ª(token, unit)å¯¹")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    if results:
        out = pd.concat(results, ignore_index=True)
        out.to_parquet(out_path, index=False)
        print(f"   âœ… å¯¼å‡ºå®Œæˆ: {len(out)} ä¸ªèšåˆè®°å½•ï¼Œä¿å­˜åˆ° {out_path}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        cat_count = (out["key_type"] == "cat").sum()
        token_count = (out["key_type"] == "token").sum()
        print(f"   ğŸ“Š èšåˆç»Ÿè®¡: category={cat_count}, token={token_count}")
    else:
        print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„èšåˆæ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒAè¡¨å‚æ•°å¯¼å‡ºæµç¨‹
    
    æ‰§è¡Œæµç¨‹ï¼š
        1. è§£æå‘½ä»¤è¡Œå‚æ•°
        2. åŠ è½½Aè¡¨å’Œfoodè¡¨æ•°æ®
        3. åˆ›å»ºè¾“å‡ºç›®å½•
        4. æ‰§è¡Œå››ä¸ªå¯¼å‡ºå‡½æ•°
        5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Aè¡¨å‚æ•°å¯¼å‡ºè„šæœ¬")
    parser.add_argument("--a_table_path", required=True, help="Aè¡¨parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--food_path", required=True, help="food_processed.parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--out_dir", required=True, help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.out_dir, exist_ok=True)
    
    print(">> Aè¡¨å‚æ•°å¯¼å‡ºå¼€å§‹...")
    print(f"   Aè¡¨è·¯å¾„: {args.a_table_path}")
    print(f"   Foodè¡¨è·¯å¾„: {args.food_path}")
    print(f"   è¾“å‡ºç›®å½•: {args.out_dir}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.a_table_path):
        raise FileNotFoundError(f"Aè¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.a_table_path}")
    if not os.path.exists(args.food_path):
        raise FileNotFoundError(f"Foodè¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.food_path}")
    
    # åŠ è½½æ•°æ®
    print("\n>> åŠ è½½æ•°æ®...")
    with tqdm(total=2, desc="åŠ è½½æ•°æ®") as pbar:
        a_df = pd.read_parquet(args.a_table_path)
        print(f"   Aè¡¨: {len(a_df)} æ¡è®°å½•")
        pbar.update(1)
        
        food_df = pd.read_parquet(args.food_path)
        print(f"   Foodè¡¨: {len(food_df)} æ¡è®°å½•")
        pbar.update(1)
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_a_cols = ["fdc_id", "unit_std", "grams_per_unit"]
    missing_a_cols = [col for col in required_a_cols if col not in a_df.columns]
    if missing_a_cols:
        raise ValueError(f"Aè¡¨ç¼ºå°‘å¿…è¦åˆ—: {missing_a_cols}")
    
    required_food_cols = ["fdc_id", "food_category_id", "description_norm"]
    missing_food_cols = [col for col in required_food_cols if col not in food_df.columns]
    if missing_food_cols:
        raise ValueError(f"Foodè¡¨ç¼ºå°‘å¿…è¦åˆ—: {missing_food_cols}")
    
    # æ‰§è¡Œå¯¼å‡º
    print("\n>> æ‰§è¡Œå¯¼å‡º...")
    
    # 1. fdcçº§ä½“ç§¯å¯†åº¦
    out_path1 = os.path.join(args.out_dir, "A_fdc_volume_density.parquet")
    export_a_fdc_volume_density(a_df, ML_PER, out_path1)
    
    # 2. fdcÃ—unitçº§ä»¶æ•°å…‹é‡
    out_path2 = os.path.join(args.out_dir, "A_fdc_piece_weight.parquet")
    export_a_fdc_piece_weight(a_df, PIECE_UNITS, out_path2)
    
    # 3. å…¨å±€ä¸­ä½æ•°
    out_path3 = os.path.join(args.out_dir, "A_unit_global_median.parquet")
    export_a_unit_global_median(a_df, out_path3)
    
    # 4. ç±»åˆ«/Tokenèšåˆ
    out_path4 = os.path.join(args.out_dir, "A_cat_token_aggregates.parquet")
    export_a_cat_token_aggregates(a_df, food_df, ML_PER, PIECE_UNITS, out_path4)
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("\n>> å¯¼å‡ºå®Œæˆï¼ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    
    output_files = [
        ("A_fdc_volume_density.parquet", "fdcçº§ä½“ç§¯å¯†åº¦"),
        ("A_fdc_piece_weight.parquet", "fdcÃ—unitçº§ä»¶æ•°å…‹é‡"),
        ("A_unit_global_median.parquet", "å…¨å±€ä¸­ä½æ•°"),
        ("A_cat_token_aggregates.parquet", "ç±»åˆ«/Tokenèšåˆ")
    ]
    
    for filename, description in output_files:
        filepath = os.path.join(args.out_dir, filename)
        if os.path.exists(filepath):
            df = pd.read_parquet(filepath)
            print(f"âœ… {description}: {len(df)} æ¡è®°å½•")
        else:
            print(f"âŒ {description}: æ–‡ä»¶æœªç”Ÿæˆ")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {args.out_dir}")
    print("=" * 60)

if __name__ == "__main__":
    main()
